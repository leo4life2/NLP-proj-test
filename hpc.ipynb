{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-4vUa5rNaYu6"
   },
   "source": [
    "# HPC Tutorial\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MbJQKfXjaS_I"
   },
   "source": [
    "Major acknowledgement to Ilia Kulikov and Richard Pang for contributions to this notebook. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RFKCHHNPSh-R"
   },
   "source": [
    "## EXTREMELY IMPORTANT NOTES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gS4taepqVDfh"
   },
   "source": [
    "Do not run compute-heavy jobs on log-in nodes!\n",
    "- HPC admins (and other HPC users) will be very upset and you might get into trouble. \n",
    "- Don't worry if you don't understand what \"compute-heavy\" or \"log-in nodes\" mean, now. But hopefully at the end of the section, you'll have a better idea!\n",
    "\n",
    "\n",
    "Do not email NYU HPC unless absolutely necessary!!!\n",
    "- Check the tutorials written by NYU HPC staff: https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/cloud-computing/hpc-bursting-to-cloud (and other pages on this site) \n",
    "- Check slurm documentation\n",
    "- Ask on Campuswire\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 1: Logging in to Greene"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf24jHglVGj2"
   },
   "source": [
    "If you're not on NYU network, then you have three options (choose one): \n",
    "- Use NYU VPN (please figure this out on your own) and directly ssh to Greene\n",
    "- Gateway (`ssh [netid]@gw.hpc.nyu.edu`) -> Greene\n",
    "- CIMS cluster (if you have access) -> Greene (if you don't have an CIMS account, don't worry---you're not at any disadvantage)\n",
    "\n",
    "If you're on the NYU network including through NYU VPN: `ssh [netid]@greene.hpc.nyu.edu`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "obPl4pVuZw_3"
   },
   "source": [
    "```\n",
    "| \\ | \\ \\ / / | | | | | | |  _ \\ / ___|\n",
    "|  \\| |\\ V /| | | | | |_| | |_) | |\n",
    "| |\\  | | | | |_| | |  _  |  __/| |___\n",
    "|_| \\_| |_|  \\___/  |_| |_|_|    \\____|\n",
    "\n",
    "  ____\n",
    " / ___|_ __ ___  ___ _ __   ___\n",
    "| |  _| '__/ _ \\/ _ \\ '_ \\ / _ \\\n",
    "| |_| | | |  __/  __/ | | |  __/\n",
    " \\____|_|  \\___|\\___|_| |_|\\___|\n",
    " ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4H8tx45YVGfA"
   },
   "source": [
    "# Part 2: Looking around the filesystem using bash"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "P_PcUMgSXPZW"
   },
   "source": [
    "By default we are given the [Bash](https://www.gnu.org/software/bash/) shell upon successful connection. Shell is an environment in which we can run our commands, programs, and shell scripts. Today we will use shell to manage files, to list content of the filesystem, to run VIM text editor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mbJ_Ga24XPbN"
   },
   "source": [
    "Please learn these commands on your own, if you're not familiar with them: `cd`, `pwd`, `rm` (as well as flags like `r` or `f`), `ls` (as well as flags like `l` or `a` or `h`), `du` (as well as flags like `h` or `s`), `cp`, `scp`. Use `man ls` to check the documentation for `ls`, for example. Some quick examples:\n",
    "- `touch <filename>`: create file with name `<filename>`.\n",
    "- rm `<filename>`: remove file with name `<filename>`.\n",
    "- cp `<fp1>` `<fp2>`: copy file with path `<fp1>` to path `<fp2>`.\n",
    "- mv `<fp1>` `<fp2>`: move file from `<fp1>` to path `<fp2>`. This is used to rename files as well.\n",
    "- cd `<path>`: change current directory to `<path>`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BikYqB19aAVg"
   },
   "source": [
    "Login node should not be used to run anything related to your computations, use it for file management (`git`, `rsync`), jobs management (`srun`, `salloc`, `sbatch`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TUDoxgfnaSae"
   },
   "source": [
    "Bash holds the set of environment variables which are used to help other software to link some libraries or helper scripts:\n",
    "```\n",
    "[wy885@log-3 ~]$ env\n",
    "LD_LIBRARY_PATH=:/share/apps/centos/8/usr/lib:/share/apps/centos/8/usr/lib64:/share/apps/centos/8/lib64\n",
    "VAST=/vast/wy885\n",
    "SSH_CONNECTION=10.16.224.49 53909 216.165.13.139 22\n",
    "ARCHIVE=/archive/wy885\n",
    "LANG=en_US.UTF-8\n",
    "HISTCONTROL=ignoredups\n",
    "HOSTNAME=log-3\n",
    "__LMOD_SET_FPATH=1\n",
    ".....\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2v4m-clDiM-m"
   },
   "source": [
    "## Important: different filesystems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tx5qVxOlaDsA"
   },
   "source": [
    "Ok. We were on Greene (corresponding to filesystem B below). There're other filesystems. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws8WjGbx-N-e"
   },
   "source": [
    "\n",
    "```\n",
    "Local ---> Greene login node ---> Greene compute node (NOT USING FOR THIS COURSE)\n",
    "                            |\n",
    "                             ---> Burst node    ---> GCP compute node\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "47MTDO4gTRNK"
   },
   "source": [
    "### Filesystem A: Local\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pB8a3EtPvuZk"
   },
   "source": [
    "For example, your laptop. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LECeIUhbvuXp"
   },
   "source": [
    "### Filesystem B: Greene"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rp3K946lvuVR"
   },
   "source": [
    "|  |\tenv  var |\twhat for |\tflushed |\tquota|\n",
    "| --- | --- | --- | --- | --- | \n",
    "| /archive\t| \\$ARCHIVE | long term storage\t| NO\t| 2TB/20K inodes |\n",
    "| /home\t| \\$HOME\t| probably nothing\t| NO\t| 50GB/30K inodes |\n",
    "| /scratch |\t\\$SCRATCH | experiments/stuff\t| YES (60 days)\t| 5TB/1M inodes |\n",
    "\n",
    "Check quota by `myquota`. \n",
    "\n",
    "For this course:\n",
    "- You probably won't be using `/archive`.  \n",
    "- You will store very very few things (maybe just a few lines of environment-related code) on `/home`.\n",
    "\n",
    "\n",
    "Where to store your data\n",
    "- `/scratch/[netid]`\n",
    "\n",
    "- How to get on Greene login node? `ssh [netid]@greene.hpc.nyu.edu` (see above).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7wl5XS0IvuRM"
   },
   "source": [
    "Where to store your temporary data (will disappear after you exit the node)?\n",
    "- `/tmp`\n",
    "\n",
    "Where to store your data you want to keep?\n",
    "- `/scratch/[netid]` (recommended)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burst: something between B and C"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to get on Burst node? After `ssh [netid]@greene.hpc.nyu.edu`, do `ssh burst`.\n",
    "- Mostly containing files from B, but not C."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem C: NYU HPC GCP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where to store your temporary data (will disappear after you exit the node)?\n",
    "- `/tmp` or `/mnt/ram`\n",
    "\n",
    "Where to store your data you want to keep?\n",
    "- `/home/[netid] ` or `/scratch/[netid]`\n",
    "\n",
    "How to get on GCP compute nodes? Our class will have one account `ds_ga_1011-2023fa`, and four partitions: `interactive`, `n1s8-v100-1`, `n1s16-v100-2` and `c2s16p`.\n",
    "\n",
    "- For simple scripts / file operations: `srun --account=ds_ga_1011-2023fa --partition=interactive --pty /bin/bash`\n",
    "  - Check hostname: this is on Google Cloud.\n",
    "  - lscpu: 1-2 CPUs.\n",
    "  - free -m: around 2GB memory.\n",
    "\n",
    "- For GPUs\n",
    "  - `srun --account=ds_ga_1011-2023fa --partition=n1s8-v100-1 --gres=gpu --time=1:00:00 --pty /bin/bash`\n",
    "\n",
    "Always use the `interactive` partition, if you're only doing very simple operations (i.e., moving files around, editing code using vim, etc.). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_NgaoiXvwSKQ"
   },
   "source": [
    "---------\n",
    "\n",
    "### How to copy files around?\n",
    "\n",
    "From A to B (you must be on NYU network; VPN also okay)\n",
    "- On A, do `scp [optional flags] [file-path] [netid]@greene.hpc.nyu.edu:[greene-destination-path]`\n",
    "\n",
    "From B to A (you must be on NYU network)\n",
    "- On A, do `scp [optional flags] [netid]@greene.hpc.nyu.edu:[file-path] [local-destination-path]`\n",
    "\n",
    "From B to C\n",
    "- On C, do `scp [optional flags] greene-dtn:[file-path] [gcp-destination-path]`\n",
    "\n",
    "From C to B\n",
    "- On C, do `scp [optional flags] [file-path] greene-dtn:[greene-destination-path]`\n",
    "\n",
    "From A to C\n",
    "- A -> B -> C\n",
    "\n",
    "From C to A\n",
    "- C -> B -> A\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hkGhDv13TRPO"
   },
   "source": [
    "# Part 3: Slurm, Burst, Singularity, and your typical workflow\n",
    "\n",
    "Slurm is very very popular in both academic and industry settings. Also singularity. So it would be good to know these tools in general. The below might seem overwhelming if it's your first time knowing about these tools, but you'll get comfortable with them very soon!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r2zHjErEb-6z"
   },
   "source": [
    "[Slurm](https://slurm.schedmd.com/documentation.html) is a job management system which allocated resources (computers) to you given your requests and also run some scripts if you pass it in.\n",
    "\n",
    "[Singularity](https://sites.google.com/nyu.edu/nyu-hpc/hpc-systems/greene/software/singularity-with-miniconda) is a software to instantiate the container-based userspace (can be seen as a virtual machine). The main idea of using a container is to provide an isolated user space on a compute node and to simplify the node management with single OS container image.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DR5UDNxSBq5J"
   },
   "source": [
    "# Part 3.1: Interactive setting \n",
    "\n",
    "The typical workflow for **interactive** (running of some script/debugging) looks like this:\n",
    "\n",
    "1. Log in: Greene’s login node.\n",
    "2. Log in to Burst node.\n",
    "3. Request a job / computational resource and wait until Slurm grants it.\n",
    "  - You always need to request a job for GPUs. \n",
    "4. Execute singularity and start container instance.\n",
    "5. Activate conda environment with your own deep learning libraries.\n",
    "6. Run your code, make changes/debugging."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PGeHgIOHTRTn"
   },
   "source": [
    "## Step 1: Log in to Greene's login node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "trBsMdW7TRWS"
   },
   "source": [
    "Described above. NEVER run compute heavy jobs on the login nodes. You'll get scolded by HPC admins and you may get into trouble."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Burst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ssh burst`\n",
    "\n",
    "Then, check our hostname by `hostname`. Do not run compute heavy jobs on this node either!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "t8IUdtb7c0Fy"
   },
   "source": [
    "## Step 3: Requesting compute node(s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On NYU HPC GCP, our class will have one account `ds_ga_1011-2023fa`, and four partitions: `interactive`, `n1s8-v100-1`, `n1s16-v100-2` and `c2s16p`.\n",
    "\n",
    "Confusing thing: One partition is called `interactive`. This `interactive` and the \"interactive setting\" in the header do not refer to the same thing.\n",
    "\n",
    "---------\n",
    "\n",
    "For simple scripts / file operations \n",
    "- `srun --account=ds_ga_1011-2023fa --partition=interactive --pty /bin/bash`\n",
    "- Check the `man` page! Or here: https://slurm.schedmd.com/srun.html\n",
    "- After getting onto GCP node:\n",
    "  - Check `hostname`: this is on Google Cloud. \n",
    "  - `lscpu`: 1-2 CPUs.\n",
    "  - `free -m`: around 2GB memory. \n",
    "\n",
    "For GPUs\n",
    "- `srun --account=ds_ga_1011-2023fa --partition=n1s8-v100-1 --gres=gpu --time=1:00:00 --pty /bin/bash`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "bash-4.4$ nvidia-smi\n",
    "Mon Feb  6 15:31:06 2023       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla V100-SXM2...  On   | 00000000:00:04.0 Off |                    0 |\n",
    "| N/A   36C    P0    40W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "h6Q-BgMnl_Kk"
   },
   "source": [
    "Check what this job looks like in the slrum queue:\n",
    "```\n",
    "bash-4.4$ squeue -u [netid]\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "            109533 n1s8-v100     bash  nhj4247  R       4:06      1 b-3-106\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UOQHNcWBg6eg"
   },
   "source": [
    "If you're cancelling your `srun`-submitted job, use control+D or `exit`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "PROBLEM: If you don't touch your keyboard for a while, or if your internet is unstable, then the job my die. A workaround is to use `tmux` or `screen`.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1YzT1_i7c0Nk"
   },
   "source": [
    "## Step 4: Setting up singularity &\n",
    "## Step 5: Activate conda environment with your own deep learning libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H4sKG-J1c0Pg"
   },
   "source": [
    "First we copy over the empty filesystem image where we will put our conda environment later (you only need to do this once semester)\n",
    "\n",
    "```\n",
    "# On Burst: first get on GCP\n",
    "srun --account=ds_ga_1011-2023fa --partition=n1s8-v100-1 --gres=gpu --time=1:00:00 --pty /bin/bash\n",
    "# Then download the overlay filesystem\n",
    "cd /scratch/[netid]\n",
    "scp greene-dtn:/scratch/work/public/overlay-fs-ext3/overlay-25GB-500K.ext3.gz .\n",
    "```\n",
    "\n",
    "Unzip the ext3 filesystem. May take 5 min here.\n",
    "```\n",
    "gunzip -vvv ./overlay-25GB-500K.ext3.gz\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-_TUmzbRc0Rm"
   },
   "source": [
    "Filesystems can be mounted as read-write (`rw`) or read-only (`ro`) when we use it with singularity.\n",
    "- read-write: use this one when setting up env (installing conda, libs, other static files)\n",
    "- read-only: use this one when running your jobs. It has to be read-only since multiple processes will access the same image. It will crash if any job has already mounted it as read-write.\n",
    "\n",
    "Now let's launch singularity container with the fresh filesystem we just copied over (you need to do the below every time you want to run GPU jobs):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AqcJOWMFc0Tn"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# On GCP (assuming our current directory is /scratch/[netid])\n",
    "# Copy the appropriate singularity image to the current working directory\n",
    "scp -rp greene-dtn:/scratch/work/public/singularity/cuda11.7.99-cudnn8.5-devel-ubuntu22.04.2.sif .\n",
    "\n",
    "singularity exec --bind /scratch --nv --overlay /scratch/zl3493/overlay-25GB-500K.ext3:rw /scratch/zl3493/cuda11.7.99-cudnn8.5-devel-ubuntu22.04.2.sif /bin/bash\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "fDMt_3QDc0Vd"
   },
   "source": [
    "**Important**: if you want to use GPUs inside singularity, add --nv argument after exec."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sOoWPmUSc0Xk"
   },
   "source": [
    "We are going to install Conda package in the `/ext3/` folder where your own filesystem is mounted.\n",
    "\n",
    "```\n",
    "## On GCP\n",
    "Singularity> cd /ext3/\n",
    "Singularity> wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "--2023-09-24 14:31:12--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8203, ...\n",
    "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 103219356 (98M) [application/x-sh]\n",
    "Saving to: 'Miniconda3-latest-Linux-x86_64.sh'\n",
    "\n",
    "Miniconda3-latest-Linux-x86_64.sh             100%[==============================================================================================>]  98.44M   186MB/s    in 0.5s\n",
    "\n",
    "2023-09-24 14:31:13 (186 MB/s) - 'Miniconda3-latest-Linux-x86_64.sh' saved [103219356/103219356]\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_3xCS0-QANII"
   },
   "source": [
    "Now install the conda package. If the installer asks you where to install, type in `/ext3/miniconda3`. Agree to change your bashrc in the end if installer asks so):\n",
    "\n",
    "```\n",
    "Singularity> bash ./Miniconda3-latest-Linux-x86_64.sh\n",
    "PREFIX=/ext3/miniconda3\n",
    "Unpacking payload ...\n",
    "```\n",
    "\n",
    "Setup conda by running the following\n",
    "```\n",
    "source /ext3/miniconda3/etc/profile.d/conda.sh\n",
    "export PATH=/ext3/miniconda3/bin:$PATH\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7b9vwy1trhCU"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Many python libraries store some static files like pretrained models on disk when you import particular model. Lets re-route the cache location to $SCRATCH (this is `/scratch/[netid]`; if the variable doesn't exist, type in `/scratch/[netid]` instead of `$SCRATCH`; or simply set `SCRATCH=/scratch/[netid]`).\n",
    "\n",
    "First, create folders in scratch: \n",
    "```\n",
    "Singularity> export SCRATCH=/scratch/[netid]\n",
    "Singularity> mkdir $SCRATCH/.cache\n",
    "Singularity> mkdir $SCRATCH/.conda\n",
    "```\n",
    "\n",
    "Now remove all existing cache:\n",
    "\n",
    "```\n",
    "Singularity> cd ~\n",
    "Singularity> rm -rfv .conda\n",
    "Singularity> rm -rfv .cache\n",
    "```\n",
    "Now create symbolic links (symlinks) to scratch:\n",
    "```\n",
    "Singularity> ln -s $SCRATCH/.conda ./\n",
    "Singularity> ln -s $SCRATCH/.cache ./\n",
    "\n",
    "Singularity> ls -l .conda\n",
    "lrwxrwxrwx. 1 wy885 wy885 21 Sep 24 14:47 .conda -> /scratch/wy885/.conda\n",
    "```\n",
    "\n",
    "Now let's install a few more libraries:\n",
    "```\n",
    "## Make sure that your conda environment is activated (e.g. run conda activate)\n",
    "pip install torch\n",
    "pip install transformers\n",
    "pip install nlp\n",
    "pip install sklearn\n",
    "```\n",
    "\n",
    "Let's check how ‘heavy’ our filesystem became:\n",
    "```\n",
    "(base) Singularity> du -sh /ext3/\n",
    "5.3G    /ext3/\n",
    "```\n",
    "\n",
    "We are capped at 25G so we are good to go. Feel free to install other packages along the way, but remember to mount filesystem with `rw`, otherwise you will get read-only errors.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sHj6_9wUBRPo"
   },
   "source": [
    "## Step 6: Run code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IKA36CVrrg_k"
   },
   "source": [
    "Lets try out some demo with transformers. Enter `python`:\n",
    "\n",
    "```\n",
    "Python 3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> import torch\n",
    ">>> x = torch.tensor([1,2])\n",
    ">>> x\n",
    "tensor([1, 2])\n",
    "```\n",
    "\n",
    "Make sure pytorch is using GPUs!\n",
    "```\n",
    "(base) Singularity> python\n",
    "Python 3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0] on linux\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    ">>> import torch\n",
    ">>> torch.cuda\n",
    "<module 'torch.cuda' from '/ext3/miniconda3/lib/python3.10/site-packages/torch/cuda/__init__.py'>\n",
    ">>> torch.cuda.is_available()\n",
    "True\n",
    ">>> \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KjRXnPPJBYXa"
   },
   "source": [
    "# Part 3.2: Batch setting\n",
    "\n",
    "Personally, for very quick experiments or for debugging, I use the interactive setting (Part 3.1). For submitting lots of experiments (maybe I want to start some experiments and check the results in the morning), I use the batch setting.\n",
    "\n",
    "The typical workflow for submitting batch jobs looks like this:\n",
    "\n",
    "1. Log in: Greene’s login node.\n",
    "2. Log in to Burst node.\n",
    "3. Submit a `sbatch` script. Within this script, do the following:\n",
    "  - Request a job / computational resource. \n",
    "  - Execute singularity and start container instance.\n",
    "  - Activate conda environment with your own deep learning libraries.\n",
    "  - Run your code, make changes/debugging."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MO66HwKHBYba"
   },
   "source": [
    "Here we consider the following python script which uses the available GPU:\n",
    "https://nyu-cs2590.github.io/course-material/spring2023/section/sec02/test_gpu.py\n",
    "\n",
    "Now we want to submit a job from login node using SLURM. The job will simply run this python script on the allocated machine, save the output from stdout and exit.\n",
    "\n",
    "Make sure your SCRATCH is set to your scratch folder: `export SCRATCH=/scratch/[netid]`\n",
    "\n",
    "First, download the actual job script.\n",
    "\n",
    "```\n",
    "# Run from the burst node\n",
    "cd $SCRATCH\n",
    "\n",
    "wget https://nyu-cs2590.github.io/course-material/fall2023/section/sec03/test_gpu.py\n",
    "```\n",
    "\n",
    "Second, download the batch submission script:\n",
    "```\n",
    "cd $SCRATCH\n",
    "\n",
    "wget https://nyu-cs2590.github.io/course-material/fall2023/section/sec03/gpu_job.slurm\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3QGwvH8q0dp0"
   },
   "source": [
    "\n",
    "We need to add the /ext3/env.sh script to your filesystem. Now we will use singularity purely for copying over the script, i.e. we are not runnning any actual computations there:\n",
    "```\n",
    "[wy885@log-burst ~]$ srun --account=ds_ga_1011-2023fa --partition=interactive --time=1:00:00 --pty /bin/bash\n",
    "bash-4.4$ singularity exec --bind /scratch --nv --overlay /scratch/zl3493/overlay-25GB-500K.ext3:rw /scratch/zl3493/cuda11.7.99-cudnn8.5-devel-ubuntu22.04.2.sif /bin/bash\n",
    "\n",
    "Singularity> \n",
    "\n",
    "Singularity> wget https://nyu-cs2590.github.io/course-material/fall2023/section/sec03/env.sh -O /ext3/env.sh\n",
    "--2023-09-24 16:09:17--  https://nyu-cs2590.github.io/course-material/fall2023/section/sec03/env.sh\n",
    "Resolving nyu-cs2590.github.io (nyu-cs2590.github.io)... 185.199.109.153, 185.199.111.153, 185.199.108.153, ...\n",
    "Connecting to nyu-cs2590.github.io (nyu-cs2590.github.io)|185.199.109.153|:443... connected.\n",
    "HTTP request sent, awaiting response... 200 OK\n",
    "Length: 98 [application/x-sh]\n",
    "Saving to: '/ext3/env.sh'\n",
    "\n",
    "/ext3/env.sh                                  100%[==============================================================================================>]      98  --.-KB/s    in 0s\n",
    "\n",
    "2023-09-24 16:09:17 (9.96 MB/s) - '/ext3/env.sh' saved [98/98]\n",
    "\n",
    "\n",
    "Singularity> cat /ext3/env.sh \n",
    "#!/bin/bash\n",
    "\n",
    "source /ext3/miniconda3/etc/profile.d/conda.sh\n",
    "export PATH=/ext3/miniconda3/bin:$PATHSingularity>\n",
    "Singularity> exit\n",
    "bash-4.4$ exit\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qHlx3opYBYZb"
   },
   "source": [
    "Now we submit the job using sbatch command `sbatch gpu_job.slurm` and check how it is running using squeue command.\n",
    "Note: make sure to change the [netid] placeholder in `gpu_job.slurm` to the correct id.\n",
    "```\n",
    "[wy885@log-burst ~]$ sbatch gpu_job.slurm\n",
    "Submitted batch job 146194\n",
    "[wy885@log-burst ~]$ squeue -u wy885\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "            146194 n1s8-v100 job_wgpu    wy885 CF       0:45      1 b-3-40\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0-8LTsyIseGG"
   },
   "source": [
    "After the job is done we check the corresponding output log (it will be in GCP filesystem, so you will need to request a node before)\n",
    "```\n",
    "bash-4.4$ cat 146194_job_wgpu.out\n",
    "Torch cuda available: True\n",
    "GPU name: Tesla V100-SXM2-16GB\n",
    "\n",
    "\n",
    "CPU matmul elapsed: 1.138293743133545 sec.\n",
    "GPU matmul elapsed: 5.04845929145813 sec.\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "In above `sbatch` script we need to use the option `requeue`. Why `requeue`? GCP uses preemption, or preemptive instances\n",
    "- Max time: 24 hours.\n",
    "- Your job might be killed (with low-to-medium probability) for no reason. This means that you need to requeue your job, and you need to automatically load from your last checkpoint. This means you'll need to write code that automatically loads from the last checkpoint (perhaps always named `checkpoint_last.pt`) -- if the file doesn't exist, initialize the network from scratch. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook example:\n",
    "- You might find it simpler to run on your laptop (you can download Anaconda which includes Jupyter notebook), if you don't need GPUs. \n",
    "- If you need GPUs, on your GCP instance, go to `/share/apps/examples/jupyter`, and edit `run-jupyter-gpu.sbatch` (change the account and the singularity image). Then, sbatch your script. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
